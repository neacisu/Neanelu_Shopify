# ============================================
# Neanelu Shopify - Docker Compose Base
# ============================================
# Fișier BASE - setări comune pentru toate mediile
# Override-urile specifice sunt în docker-compose.dev.yml
#
# Utilizare dev: docker compose -f docker-compose.yml -f docker-compose.dev.yml up -d
# Sau simplu: pnpm db:up

services:
  # ============================================
  # Traefik - Reverse Proxy (TLS/ACME)
  # ============================================
  traefik:
    image: traefik:v3.6.6
    container_name: neanelu_traefik
    restart: unless-stopped
    command:
      - --api.dashboard=true
      - --api.insecure=false
      - --providers.docker=true
      - --providers.docker.exposedbydefault=false
      - --providers.docker.constraints=Label(`com.docker.compose.project`,`neanelu_shopify`)
      - --entrypoints.web.address=:80
      - --entrypoints.web.http.redirections.entrypoint.to=websecure
      - --entrypoints.web.http.redirections.entrypoint.scheme=https
      - --entrypoints.websecure.address=:443
      - --certificatesresolvers.${TRAEFIK_CERT_RESOLVER:?TRAEFIK_CERT_RESOLVER is required}.acme.email=${TRAEFIK_ACME_EMAIL:?TRAEFIK_ACME_EMAIL is required}
      - --certificatesresolvers.${TRAEFIK_CERT_RESOLVER:?TRAEFIK_CERT_RESOLVER is required}.acme.storage=/letsencrypt/acme.json
      - --certificatesresolvers.${TRAEFIK_CERT_RESOLVER:?TRAEFIK_CERT_RESOLVER is required}.acme.httpchallenge=true
      - --certificatesresolvers.${TRAEFIK_CERT_RESOLVER:?TRAEFIK_CERT_RESOLVER is required}.acme.httpchallenge.entrypoint=web
    ports:
      - '${TRAEFIK_HTTP_PORT:-80}:80'
      - '${TRAEFIK_HTTPS_PORT:-443}:443'
    volumes:
      - traefik_letsencrypt:/letsencrypt
      - /var/run/docker.sock:/var/run/docker.sock:ro
    networks:
      - public_net
      - neanelu_frontend_network
    labels:
      - traefik.enable=true
      # Dashboard (requires basic auth)
      - traefik.http.routers.traefik.rule=Host(`${APP_HOSTNAME:?APP_HOSTNAME is required}`) && PathPrefix(`/traefik`)
      - traefik.http.routers.traefik.priority=1000
      - traefik.http.routers.traefik.entrypoints=websecure
      - traefik.http.routers.traefik.tls.certresolver=${TRAEFIK_CERT_RESOLVER:?TRAEFIK_CERT_RESOLVER is required}
      - traefik.http.routers.traefik.service=api@internal
      - traefik.http.middlewares.traefik-auth.basicauth.users=${TRAEFIK_DASHBOARD_USERS:?TRAEFIK_DASHBOARD_USERS is required}
      - traefik.http.middlewares.traefik-strip.stripprefix.prefixes=/traefik
      - traefik.http.middlewares.traefik-strip.stripprefix.forceSlash=false
      - traefik.http.routers.traefik.middlewares=traefik-strip,traefik-auth
    healthcheck:
      test: ['CMD-SHELL', 'nc -z 127.0.0.1 80 && nc -z 127.0.0.1 443']
      interval: 10s
      timeout: 3s
      retries: 10
      start_period: 15s

  # ============================================
  # PostgreSQL 18 + pgvector - Baza de date principală
  # ============================================
  # Folosim imaginea oficială pgvector/pgvector:pg18 (include pgvector 0.8.1+)
  db:
    build:
      context: ./docker/postgres
      dockerfile: Dockerfile
    image: neanelu-postgres:pg18-pgvector
    container_name: neanelu_postgres
    restart: unless-stopped
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
      # Optimizări pentru JSONB și ingestie masivă (PostgreSQL 18.1)
      POSTGRES_INITDB_ARGS: '--data-checksums'
    volumes:
      # PostgreSQL 18+ folosește subdirectoare per versiune
      # Mount la /var/lib/postgresql pentru compatibilitate pg_upgrade
      - postgres_data:/var/lib/postgresql
    networks:
      - neanelu_network
    healthcheck:
      test: ['CMD-SHELL', 'pg_isready -U $$POSTGRES_USER -d $$POSTGRES_DB']
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    # Optimizări PostgreSQL 18.1 pentru JSONB și COPY FROM STDIN
    command: >
      postgres
      -c shared_buffers=256MB
      -c work_mem=64MB
      -c maintenance_work_mem=128MB
      -c effective_cache_size=512MB
      -c max_connections=200
      -c log_statement=none
      -c log_min_duration_statement=1000

  # ============================================
  # Redis 8.4 - Cozi BullMQ, Cache, Vector Search
  # ============================================
  # NOTĂ: Redis 8.4 include NATIV modulele RediSearch și RedisJSON
  # Nu mai este nevoie de redis-stack (deprecated Dec 2025)
  redis:
    image: redis:8.4
    container_name: neanelu_redis
    restart: unless-stopped
    volumes:
      - redis_data:/data
    networks:
      - neanelu_network
    healthcheck:
      test: ['CMD', 'redis-cli', 'ping']
      interval: 10s
      timeout: 5s
      retries: 5
    # Configurare Redis 8.4 pentru BullMQ + Vector Search
    command: >
      redis-server
      --appendonly yes
      --appendfsync everysec
      --maxmemory 512mb
      --maxmemory-policy noeviction
      --tcp-keepalive 300

  # ============================================
  # Jaeger - Observabilitate/Tracing (OpenTelemetry)
  # ============================================
  jaeger:
    image: jaegertracing/jaeger:2.14.1
    container_name: neanelu_jaeger
    restart: unless-stopped
    networks:
      - neanelu_network
    healthcheck:
      test: ['CMD-SHELL', 'wget -qO- http://127.0.0.1:16686/ >/dev/null 2>&1']
      interval: 10s
      timeout: 3s
      retries: 15
      start_period: 20s

  # ============================================
  # OpenTelemetry Collector - Receives OTLP from apps
  # ============================================
  otel-collector:
    image: otel/opentelemetry-collector-contrib:0.142.0
    container_name: neanelu_otel_collector
    restart: unless-stopped
    command: ['--config=/etc/otelcol/config.yaml']
    volumes:
      - ./docker/otel-collector/config.yaml:/etc/otelcol/config.yaml:ro
    networks:
      - neanelu_network
    depends_on:
      - jaeger
    healthcheck:
      # Image does not include a shell; keep this as a lightweight binary check.
      test: ['CMD', '/otelcol-contrib', '--version']
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s

  # ============================================
  # Backend API (Fastify) - Containerized
  # ============================================
  backend-worker:
    build:
      context: .
      dockerfile: apps/backend-worker/Dockerfile
    image: neanelu-backend-worker:dev
    container_name: neanelu_backend_worker
    restart: unless-stopped
    environment:
      PORT: ${PORT}
      NODE_ENV: ${NODE_ENV}
      APP_HOST: ${APP_HOST}
      DATABASE_URL: ${DATABASE_URL_DOCKER}
      REDIS_URL: ${REDIS_URL_DOCKER}
      SHOPIFY_API_KEY: ${SHOPIFY_API_KEY}
      SHOPIFY_API_SECRET: ${SHOPIFY_API_SECRET}
      SCOPES: ${SCOPES}
      ENCRYPTION_KEY_VERSION: ${ENCRYPTION_KEY_VERSION}
      ENCRYPTION_KEY_256: ${ENCRYPTION_KEY_256}
      NPM_TASKFORCESH_TOKEN: ${NPM_TASKFORCESH_TOKEN}
      BULLMQ_PRO_TOKEN: ${BULLMQ_PRO_TOKEN}
      OTEL_SERVICE_NAME: ${OTEL_SERVICE_NAME}
      OTEL_EXPORTER_OTLP_ENDPOINT: ${OTEL_EXPORTER_OTLP_ENDPOINT_DOCKER}
      OTEL_SAMPLING_RATIO: ${OTEL_SAMPLING_RATIO}
      OBS_DEBUG: ${OBS_DEBUG}
    volumes:
      - backend_worker_node_modules:/app/node_modules
      - backend_worker_pnpm_store:/pnpm/store
    depends_on:
      db:
        condition: service_healthy
      redis:
        condition: service_healthy
      otel-collector:
        condition: service_started
    networks:
      - neanelu_network
      - neanelu_frontend_network
    labels:
      - traefik.enable=true
      - traefik.docker.network=neanelu_frontend_network

      # /api/* public prefix (prod) -> forward as-is to backend (backend registers routes under /api/*)
      - traefik.http.routers.backend-api.rule=Host(`${APP_HOSTNAME:?APP_HOSTNAME is required}`) && PathPrefix(`/api`)
      - traefik.http.routers.backend-api.priority=300
      - traefik.http.routers.backend-api.entrypoints=websecure
      - traefik.http.routers.backend-api.tls.certresolver=${TRAEFIK_CERT_RESOLVER:?TRAEFIK_CERT_RESOLVER is required}
      - traefik.http.routers.backend-api.service=backend

      - traefik.http.routers.backend.rule=Host(`${APP_HOSTNAME:?APP_HOSTNAME is required}`)
      - traefik.http.routers.backend.priority=100
      - traefik.http.routers.backend.entrypoints=websecure
      - traefik.http.routers.backend.tls.certresolver=${TRAEFIK_CERT_RESOLVER:?TRAEFIK_CERT_RESOLVER is required}
      - traefik.http.services.backend.loadbalancer.server.port=${PORT}
    healthcheck:
      test:
        [
          'CMD',
          'node',
          '-e',
          'const http=require("http"); const port=process.env.PORT||65000; const req=http.get({host:"127.0.0.1", port, path:"/health/ready", timeout:2000}, (res)=>{process.exit(res.statusCode===200?0:1)}); req.on("timeout",()=>{req.destroy(); process.exit(1)}); req.on("error",()=>process.exit(1));',
        ]
      interval: 10s
      timeout: 5s
      retries: 30
      start_period: 120s

  # ============================================
  # PLACEHOLDERS (future implementation)
  # ============================================
  web-admin:
    build:
      context: .
      dockerfile: apps/web-admin/Dockerfile
    image: neanelu-web-admin:dev
    container_name: neanelu_web_admin
    restart: unless-stopped
    environment:
      NODE_ENV: ${NODE_ENV}
      FRONTEND_PORT: ${FRONTEND_PORT:-65001}
      APP_HOSTNAME: ${APP_HOSTNAME}
      NPM_TASKFORCESH_TOKEN: ${NPM_TASKFORCESH_TOKEN}
      # Vite public env (embedded Shopify App Bridge)
      VITE_SHOPIFY_API_KEY: ${SHOPIFY_API_KEY}
    networks:
      - neanelu_frontend_network
    labels:
      - traefik.enable=true
      - traefik.docker.network=neanelu_frontend_network
      - traefik.http.routers.web-admin.rule=Host(`${APP_HOSTNAME:?APP_HOSTNAME is required}`) && PathPrefix(`/app`)
      - traefik.http.routers.web-admin.priority=200
      - traefik.http.routers.web-admin.entrypoints=websecure
      - traefik.http.routers.web-admin.tls.certresolver=${TRAEFIK_CERT_RESOLVER:?TRAEFIK_CERT_RESOLVER is required}
      - traefik.http.services.web-admin.loadbalancer.server.port=${FRONTEND_PORT:-65001}

      # Shopify may open the app at the host root (/ ?shop=...&host=...)
      # while our SPA is mounted under /app. Redirect root requests to /app/.
      - traefik.http.routers.web-admin-root.rule=Host(`${APP_HOSTNAME:?APP_HOSTNAME is required}`) && Path(`/`)
      - traefik.http.routers.web-admin-root.priority=250
      - traefik.http.routers.web-admin-root.entrypoints=websecure
      - traefik.http.routers.web-admin-root.tls.certresolver=${TRAEFIK_CERT_RESOLVER:?TRAEFIK_CERT_RESOLVER is required}
      - traefik.http.routers.web-admin-root.service=web-admin
      - traefik.http.routers.web-admin-root.middlewares=web-admin-root-redirect
      - traefik.http.middlewares.web-admin-root-redirect.redirectregex.regex=^/$$
      - traefik.http.middlewares.web-admin-root-redirect.redirectregex.replacement=/app/
      - traefik.http.middlewares.web-admin-root-redirect.redirectregex.permanent=false
    healthcheck:
      test:
        [
          'CMD',
          'node',
          '-e',
          'const http=require("http"); const port=process.env.FRONTEND_PORT||65001; const req=http.get({host:"127.0.0.1", port, path:"/", timeout:2000}, (res)=>{process.exit(res.statusCode>=200 && res.statusCode<500 ? 0 : 1)}); req.on("timeout",()=>{req.destroy(); process.exit(1)}); req.on("error",()=>process.exit(1));',
        ]
      interval: 10s
      timeout: 5s
      retries: 30
      start_period: 120s

  bull-board:
    image: node:24-bookworm-slim
    container_name: neanelu_bull_board
    restart: unless-stopped
    profiles: ['future']
    command: ['sh', '-c', "echo 'bull-board placeholder (not implemented yet)' && sleep infinity"]
    networks:
      - neanelu_network
    healthcheck:
      test: ['CMD', 'node', '-e', 'process.exit(0)']
      interval: 30s
      timeout: 3s
      retries: 3
      start_period: 5s

  # ============================================
  # Prometheus - Metrics storage
  # ============================================
  prometheus:
    image: prom/prometheus:v3.8.1
    container_name: neanelu_prometheus
    restart: unless-stopped
    command:
      - --config.file=/etc/prometheus/prometheus.yml
      - --storage.tsdb.path=/prometheus
    volumes:
      - ./docker/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    networks:
      - neanelu_network
    healthcheck:
      test: ['CMD-SHELL', 'wget -qO- http://127.0.0.1:9090/-/ready >/dev/null 2>&1']
      interval: 10s
      timeout: 3s
      retries: 15
      start_period: 20s

  # ============================================
  # Alertmanager - Alerts routing
  # ============================================
  alertmanager:
    image: prom/alertmanager:v0.30.0
    container_name: neanelu_alertmanager
    restart: unless-stopped
    command:
      - --config.file=/etc/alertmanager/alertmanager.yml
    volumes:
      - ./docker/alertmanager/alertmanager.yml:/etc/alertmanager/alertmanager.yml:ro
    networks:
      - neanelu_network
    healthcheck:
      test: ['CMD-SHELL', 'wget -qO- http://127.0.0.1:9093/-/ready >/dev/null 2>&1']
      interval: 10s
      timeout: 3s
      retries: 15
      start_period: 20s

  # ============================================
  # Loki - Logs aggregation
  # ============================================
  loki:
    image: grafana/loki:3.6.3
    container_name: neanelu_loki
    restart: unless-stopped
    command: ['-config.file=/etc/loki/loki-config.yml']
    volumes:
      - ./docker/loki/loki-config.yml:/etc/loki/loki-config.yml:ro
      - loki_data:/loki
    networks:
      - neanelu_network
    healthcheck:
      # Image is distroless (no shell). Keep this as a lightweight binary check.
      test: ['CMD', '/usr/bin/loki', '-version']
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s

  # ============================================
  # Promtail - Ships Docker logs to Loki
  # ============================================
  promtail:
    image: grafana/promtail:3.6.3
    container_name: neanelu_promtail
    restart: unless-stopped
    command: ['-config.file=/etc/promtail/promtail-config.yml']
    volumes:
      - ./docker/promtail/promtail-config.yml:/etc/promtail/promtail-config.yml:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - promtail_positions:/tmp
    networks:
      - neanelu_network
    depends_on:
      - loki
    healthcheck:
      test: ['CMD-SHELL', 'pgrep -f promtail >/dev/null']
      interval: 10s
      timeout: 3s
      retries: 15
      start_period: 20s

  # ============================================
  # Grafana - Dashboards
  # ============================================
  grafana:
    image: grafana/grafana:12.3.1
    container_name: neanelu_grafana
    restart: unless-stopped
    environment:
      GF_SERVER_DOMAIN: ${APP_HOSTNAME:?APP_HOSTNAME is required}
      GF_SERVER_ROOT_URL: ${APP_HOST:?APP_HOST is required}
      GF_SECURITY_ADMIN_USER: ${GRAFANA_ADMIN_USER:-admin}
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_ADMIN_PASSWORD:-admin}
    volumes:
      - grafana_data:/var/lib/grafana
      - ./docker/grafana/provisioning:/etc/grafana/provisioning:ro
    networks:
      - neanelu_network
    healthcheck:
      test: ['CMD-SHELL', 'curl -fsS http://127.0.0.1:3000/api/health >/dev/null 2>&1']
      interval: 10s
      timeout: 5s
      retries: 15
      start_period: 40s

# ============================================
# VOLUMES (date persistente)
# ============================================
volumes:
  postgres_data:
    name: neanelu_postgres_data
  redis_data:
    name: neanelu_redis_data
  traefik_letsencrypt:
    name: neanelu_traefik_letsencrypt
  prometheus_data:
    name: neanelu_prometheus_data
  grafana_data:
    name: neanelu_grafana_data
  loki_data:
    name: neanelu_loki_data
  promtail_positions:
    name: neanelu_promtail_positions
  backend_worker_node_modules:
    name: neanelu_backend_worker_node_modules
  backend_worker_pnpm_store:
    name: neanelu_backend_worker_pnpm_store

# ============================================
# NETWORKS
# ============================================
networks:
  public_net:
    name: neanelu_public_net
    driver: bridge
  neanelu_frontend_network:
    name: neanelu_frontend_network
    driver: bridge
  neanelu_network:
    name: neanelu_network
    driver: bridge
